{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "########################################################################\n",
    "# Python Standard Libraries\n",
    "import os\n",
    "import multiprocessing\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "########################################################################\n",
    "# Numpy Library\n",
    "import numpy as np # linear algebra\n",
    "\n",
    "########################################################################\n",
    "# Pandas Library\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "########################################################################\n",
    "# MATPLOT Library\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "%matplotlib inline\n",
    "\n",
    "########################################################################\n",
    "# SKLearn Library\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_curve, classification_report, confusion_matrix, average_precision_score, roc_curve, auc, multilabel_confusion_matrix\n",
    "\n",
    "########################################################################\n",
    "# SCIPY Library\n",
    "from scipy.stats import gaussian_kde\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM:                   18.621841 GB\n",
      "CORES:                 4\n",
      "Architecture:        x86_64\r\n",
      "CPU op-mode(s):      32-bit, 64-bit\r\n",
      "Byte Order:          Little Endian\r\n",
      "CPU(s):              4\r\n",
      "On-line CPU(s) list: 0-3\r\n",
      "Thread(s) per core:  2\r\n",
      "Core(s) per socket:  2\r\n",
      "Socket(s):           1\r\n",
      "NUMA node(s):        1\r\n",
      "Vendor ID:           GenuineIntel\r\n",
      "CPU family:          6\r\n",
      "Model:               85\r\n",
      "Model name:          Intel(R) Xeon(R) CPU @ 2.00GHz\r\n",
      "Stepping:            3\r\n",
      "CPU MHz:             2000.122\r\n",
      "BogoMIPS:            4000.24\r\n",
      "Hypervisor vendor:   KVM\r\n",
      "Virtualization type: full\r\n",
      "L1d cache:           32K\r\n",
      "L1i cache:           32K\r\n",
      "L2 cache:            1024K\r\n",
      "L3 cache:            39424K\r\n",
      "NUMA node0 CPU(s):   0-3\r\n",
      "Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\r\n"
     ]
    }
   ],
   "source": [
    "# Utility functions\n",
    "########################################################################\n",
    "# Print system information\n",
    "def print_system_info():\n",
    "    mem_bytes = os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES')  # e.g. 4015976448\n",
    "    mem_gib = mem_bytes/(1024.**3)  # e.g. 3.74\n",
    "    print(\"{:<23}{:f} GB\".format('RAM:', mem_gib))\n",
    "    print(\"{:<23}{:d}\".format('CORES:', multiprocessing.cpu_count()))\n",
    "    !lscpu\n",
    "\n",
    "########################################################################\n",
    "# Walk through input files\n",
    "def print_input_files():\n",
    "    # Input data files are available in the \"../input/\" directory.\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "        for filename in filenames:\n",
    "            print(os.path.join(dirname, filename))\n",
    "\n",
    "########################################################################\n",
    "# Dump text files\n",
    "def dump_text_file(fname):\n",
    "    with open(fname, 'r') as f:\n",
    "        print(f.read())\n",
    "\n",
    "########################################################################\n",
    "# Dump CSV files\n",
    "def dump_csv_file(fname, count=5):\n",
    "    # count: 0 - column names only, -1 - all rows, default = 5 rows max\n",
    "    df = pd.read_csv(fname)\n",
    "    if count < 0:\n",
    "        count = df.shape[0]\n",
    "    return df.head(count)\n",
    "\n",
    "########################################################################\n",
    "# Dataset related functions\n",
    "ds_nbaiot = '/kaggle/input/nbaiot-dataset'\n",
    "dn_nbaiot = ['Danmini_Doorbell', 'Ecobee_Thermostat', 'Ennio_Doorbell', 'Philips_B120N10_Baby_Monitor', 'Provision_PT_737E_Security_Camera', 'Provision_PT_838_Security_Camera', 'Samsung_SNH_1011_N_Webcam', 'SimpleHome_XCS7_1002_WHT_Security_Camera', 'SimpleHome_XCS7_1003_WHT_Security_Camera']\n",
    "\n",
    "def fname(ds, f):\n",
    "    if '.csv' not in f:\n",
    "        f = f'{f}.csv'\n",
    "    return os.path.join(ds, f)\n",
    "\n",
    "def fname_nbaiot(f):\n",
    "    return fname(ds_nbaiot, f)\n",
    "\n",
    "def get_nbaiot_device_files():\n",
    "    nbaiot_all_files = dump_csv_file(fname_nbaiot('data_summary'), -1)\n",
    "    nbaiot_all_files = nbaiot_all_files.iloc[:,0:1].values\n",
    "    device_id = 1\n",
    "    indices = []\n",
    "    for j in range(len(nbaiot_all_files)):\n",
    "        if str(device_id) not in str(nbaiot_all_files[j]):\n",
    "            indices.append(j)\n",
    "            device_id += 1\n",
    "    nbaiot_device_files = np.split(nbaiot_all_files, indices)\n",
    "    return nbaiot_device_files\n",
    "\n",
    "def get_nbaiot_device_data(device_id, count_norm=-1, count_anom=-1):\n",
    "    if device_id < 1 or device_id > 9:\n",
    "        assert False, \"Please provide a valid device ID 1-9, both inclusive\"\n",
    "    if count_anom == -1:\n",
    "        count_anom = count_norm\n",
    "    device_index = device_id -1\n",
    "    device_files = get_nbaiot_device_files()\n",
    "    device_file = device_files[device_index]\n",
    "    df = pd.DataFrame()\n",
    "    y = []\n",
    "    for i in range(len(device_file)):\n",
    "        fname = str(device_file[i][0])\n",
    "        df_c = pd.read_csv(fname_nbaiot(fname))\n",
    "        count = count_anom\n",
    "        if 'benign' in fname:\n",
    "            count = count_norm\n",
    "        rows = count if count >=0 else df_c.shape[0]\n",
    "        print(\"processing\", fname, \"rows =\", rows)\n",
    "        y_np = np.ones(rows) if 'benign' in fname else np.zeros(rows)\n",
    "        y.extend(y_np.tolist())\n",
    "        df = pd.concat([df.iloc[:,:].reset_index(drop=True),\n",
    "                      df_c.iloc[:rows,:].reset_index(drop=True)], axis=0)\n",
    "    X = df.iloc[:,:].values\n",
    "    y = np.array(y)\n",
    "    Xdf = df\n",
    "    return (X, y, Xdf)\n",
    "\n",
    "def get_nbaiot_devices_data():\n",
    "    devices_data = []\n",
    "    for i in range(9):\n",
    "        device_id = i + 1\n",
    "        (X, y) = get_nbaiot_device_data(device_id)\n",
    "        devices_data.append((X, y))\n",
    "    return devices_data\n",
    "#print_input_files()\n",
    "print_system_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********************************************************************************************************\n",
    "# This is a third party implementation\n",
    "# https://github.com/JustGlowing/minisom\n",
    "# MiniSom by Giuseppe Vettigli is licensed under the Creative Commons Attribution 3.0 Unported License.\n",
    "# To view a copy of this license, visit http://creativecommons.org/licenses/by/3.0/.\n",
    "# ********************************************************************************************************\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "from numpy import (array, unravel_index, nditer, linalg, random, subtract,\n",
    "                   power, exp, pi, zeros, arange, outer, meshgrid, dot)\n",
    "from collections import defaultdict\n",
    "from warnings import warn\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Minimalistic implementation of the Self Organizing Maps (SOM).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def fast_norm(x):\n",
    "    \"\"\"Returns norm-2 of a 1-D numpy array.\n",
    "    * faster than linalg.norm in case of 1-D arrays (numpy 1.9.2rc1).\n",
    "    \"\"\"\n",
    "    return sqrt(dot(x, x.T))\n",
    "\n",
    "\n",
    "class MiniSom(object):\n",
    "    def __init__(self, x, y, input_len, sigma=1.0, learning_rate=0.5, decay_function=None, random_seed=None):\n",
    "        \"\"\"\n",
    "            Initializes a Self Organizing Maps.\n",
    "            x,y - dimensions of the SOM\n",
    "            input_len - number of the elements of the vectors in input\n",
    "            sigma - spread of the neighborhood function (Gaussian), needs to be adequate to the dimensions of the map.\n",
    "            (at the iteration t we have sigma(t) = sigma / (1 + t/T) where T is #num_iteration/2)\n",
    "            learning_rate - initial learning rate\n",
    "            (at the iteration t we have learning_rate(t) = learning_rate / (1 + t/T) where T is #num_iteration/2)\n",
    "            decay_function, function that reduces learning_rate and sigma at each iteration\n",
    "                            default function: lambda x,current_iteration,max_iter: x/(1+current_iteration/max_iter)\n",
    "            random_seed, random seed to use.\n",
    "        \"\"\"\n",
    "        if sigma >= x/2.0 or sigma >= y/2.0:\n",
    "            warn('Warning: sigma is too high for the dimension of the map.')\n",
    "        if random_seed:\n",
    "            self.random_generator = random.RandomState(random_seed)\n",
    "        else:\n",
    "            self.random_generator = random.RandomState(random_seed)\n",
    "        if decay_function:\n",
    "            self._decay_function = decay_function\n",
    "        else:\n",
    "            self._decay_function = lambda x, t, max_iter: x/(1+t/max_iter)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.sigma = sigma\n",
    "        self.weights = self.random_generator.rand(x,y,input_len)*2-1 # random initialization\n",
    "        for i in range(x):\n",
    "            for j in range(y):\n",
    "                self.weights[i,j] = self.weights[i,j] / fast_norm(self.weights[i,j]) # normalization\n",
    "        self.activation_map = zeros((x,y))\n",
    "        self.neigx = arange(x)\n",
    "        self.neigy = arange(y) # used to evaluate the neighborhood function\n",
    "        self.neighborhood = self.gaussian\n",
    "\n",
    "    def _activate(self, x):\n",
    "        \"\"\" Updates matrix activation_map, in this matrix the element i,j is the response of the neuron i,j to x \"\"\"\n",
    "        s = subtract(x, self.weights) # x - w\n",
    "        it = nditer(self.activation_map, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            self.activation_map[it.multi_index] = fast_norm(s[it.multi_index])  # || x - w ||\n",
    "            it.iternext()\n",
    "\n",
    "    def activate(self, x):\n",
    "        \"\"\" Returns the activation map to x \"\"\"\n",
    "        self._activate(x)\n",
    "        return self.activation_map\n",
    "\n",
    "    def gaussian(self, c, sigma):\n",
    "        \"\"\" Returns a Gaussian centered in c \"\"\"\n",
    "        d = 2*pi*sigma*sigma\n",
    "        ax = exp(-power(self.neigx-c[0], 2)/d)\n",
    "        ay = exp(-power(self.neigy-c[1], 2)/d)\n",
    "        return outer(ax, ay)  # the external product gives a matrix\n",
    "\n",
    "    def diff_gaussian(self, c, sigma):\n",
    "        \"\"\" Mexican hat centered in c (unused) \"\"\"\n",
    "        xx, yy = meshgrid(self.neigx, self.neigy)\n",
    "        p = power(xx-c[0], 2) + power(yy-c[1], 2)\n",
    "        d = 2*pi*sigma*sigma\n",
    "        return exp(-p/d)*(1-2/d*p)\n",
    "\n",
    "    def winner(self, x):\n",
    "        \"\"\" Computes the coordinates of the winning neuron for the sample x \"\"\"\n",
    "        self._activate(x)\n",
    "        return unravel_index(self.activation_map.argmin(), self.activation_map.shape)\n",
    "\n",
    "    def update(self, x, win, t):\n",
    "        \"\"\"\n",
    "            Updates the weights of the neurons.\n",
    "            x - current pattern to learn\n",
    "            win - position of the winning neuron for x (array or tuple).\n",
    "            t - iteration index\n",
    "        \"\"\"\n",
    "        eta = self._decay_function(self.learning_rate, t, self.T)\n",
    "        sig = self._decay_function(self.sigma, t, self.T) # sigma and learning rate decrease with the same rule\n",
    "        g = self.neighborhood(win, sig)*eta # improves the performances\n",
    "        it = nditer(g, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            # eta * neighborhood_function * (x-w)\n",
    "            self.weights[it.multi_index] += g[it.multi_index]*(x-self.weights[it.multi_index])\n",
    "            # normalization\n",
    "            self.weights[it.multi_index] = self.weights[it.multi_index] / fast_norm(self.weights[it.multi_index])\n",
    "            it.iternext()\n",
    "\n",
    "    def quantization(self, data):\n",
    "        \"\"\" Assigns a code book (weights vector of the winning neuron) to each sample in data. \"\"\"\n",
    "        q = zeros(data.shape)\n",
    "        for i, x in enumerate(data):\n",
    "            q[i] = self.weights[self.winner(x)]\n",
    "        return q\n",
    "\n",
    "    def random_weights_init(self, data):\n",
    "        \"\"\" Initializes the weights of the SOM picking random samples from data \"\"\"\n",
    "        it = nditer(self.activation_map, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            self.weights[it.multi_index] = data[self.random_generator.randint(len(data))]\n",
    "            self.weights[it.multi_index] = self.weights[it.multi_index]/fast_norm(self.weights[it.multi_index])\n",
    "            it.iternext()\n",
    "\n",
    "    def train_random(self, data, num_iteration):\n",
    "        \"\"\" Trains the SOM picking samples at random from data \"\"\"\n",
    "        self._init_T(num_iteration)\n",
    "        for iteration in range(num_iteration):\n",
    "            rand_i = self.random_generator.randint(len(data)) # pick a random sample\n",
    "            self.update(data[rand_i], self.winner(data[rand_i]), iteration)\n",
    "\n",
    "    def train_batch(self, data, num_iteration):\n",
    "        \"\"\" Trains using all the vectors in data sequentially \"\"\"\n",
    "        self._init_T(len(data)*num_iteration)\n",
    "        iteration = 0\n",
    "        while iteration < num_iteration:\n",
    "            idx = iteration % (len(data)-1)\n",
    "            self.update(data[idx], self.winner(data[idx]), iteration)\n",
    "            iteration += 1\n",
    "\n",
    "    def _init_T(self, num_iteration):\n",
    "        \"\"\" Initializes the parameter T needed to adjust the learning rate \"\"\"\n",
    "        self.T = num_iteration/2  # keeps the learning rate nearly constant for the last half of the iterations\n",
    "\n",
    "    def distance_map(self):\n",
    "        \"\"\" Returns the distance map of the weights.\n",
    "            Each cell is the normalised sum of the distances between a neuron and its neighbours.\n",
    "        \"\"\"\n",
    "        um = zeros((self.weights.shape[0], self.weights.shape[1]))\n",
    "        it = nditer(um, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            for ii in range(it.multi_index[0]-1, it.multi_index[0]+2):\n",
    "                for jj in range(it.multi_index[1]-1, it.multi_index[1]+2):\n",
    "                    if ii >= 0 and ii < self.weights.shape[0] and jj >= 0 and jj < self.weights.shape[1]:\n",
    "                        um[it.multi_index] += fast_norm(self.weights[ii, jj, :]-self.weights[it.multi_index])\n",
    "            it.iternext()\n",
    "        um = um/um.max()\n",
    "        return um\n",
    "\n",
    "    def activation_response(self, data):\n",
    "        \"\"\"\n",
    "            Returns a matrix where the element i,j is the number of times\n",
    "            that the neuron i,j have been winner.\n",
    "        \"\"\"\n",
    "        a = zeros((self.weights.shape[0], self.weights.shape[1]))\n",
    "        for x in data:\n",
    "            a[self.winner(x)] += 1\n",
    "        return a\n",
    "\n",
    "    def quantization_error(self, data):\n",
    "        \"\"\"\n",
    "            Returns the quantization error computed as the average distance between\n",
    "            each input sample and its best matching unit.\n",
    "        \"\"\"\n",
    "        error = 0\n",
    "        for x in data:\n",
    "            error += fast_norm(x-self.weights[self.winner(x)])\n",
    "        return error/len(data)\n",
    "\n",
    "    def win_map(self, data):\n",
    "        \"\"\"\n",
    "            Returns a dictionary wm where wm[(i,j)] is a list with all the patterns\n",
    "            that have been mapped in the position i,j.\n",
    "        \"\"\"\n",
    "        winmap = defaultdict(list)\n",
    "        for x in data:\n",
    "            winmap[self.winner(x)].append(x)\n",
    "        return winmap\n",
    "\n",
    "### unit tests\n",
    "from numpy.testing import assert_almost_equal, assert_array_almost_equal, assert_array_equal\n",
    "\n",
    "\n",
    "class TestMinisom:\n",
    "    def setup_method(self, method):\n",
    "        self.som = MiniSom(5, 5, 1)\n",
    "        for i in range(5):\n",
    "            for j in range(5):\n",
    "                assert_almost_equal(1.0, linalg.norm(self.som.weights[i,j]))  # checking weights normalization\n",
    "        self.som.weights = zeros((5, 5))  # fake weights\n",
    "        self.som.weights[2, 3] = 5.0\n",
    "        self.som.weights[1, 1] = 2.0\n",
    "\n",
    "    def test_decay_function(self):\n",
    "        assert self.som._decay_function(1., 2., 3.) == 1./(1.+2./3.)\n",
    "\n",
    "    def test_fast_norm(self):\n",
    "        assert fast_norm(array([1, 3])) == sqrt(1+9)\n",
    "\n",
    "    def test_gaussian(self):\n",
    "        bell = self.som.gaussian((2, 2), 1)\n",
    "        assert bell.max() == 1.0\n",
    "        assert bell.argmax() == 12  # unravel(12) = (2,2)\n",
    "\n",
    "    def test_win_map(self):\n",
    "        winners = self.som.win_map([5.0, 2.0])\n",
    "        assert winners[(2, 3)][0] == 5.0\n",
    "        assert winners[(1, 1)][0] == 2.0\n",
    "\n",
    "    def test_activation_reponse(self):\n",
    "        response = self.som.activation_response([5.0, 2.0])\n",
    "        assert response[2, 3] == 1\n",
    "        assert response[1, 1] == 1\n",
    "\n",
    "    def test_activate(self):\n",
    "        assert self.som.activate(5.0).argmin() == 13.0  # unravel(13) = (2,3)\n",
    "\n",
    "    def test_quantization_error(self):\n",
    "        self.som.quantization_error([5, 2]) == 0.0\n",
    "        self.som.quantization_error([4, 1]) == 0.5\n",
    "\n",
    "    def test_quantization(self):\n",
    "        q = self.som.quantization(array([4, 2]))\n",
    "        assert q[0] == 5.0\n",
    "        assert q[1] == 2.0\n",
    "\n",
    "    def test_random_seed(self):\n",
    "        som1 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        som2 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        assert_array_almost_equal(som1.weights, som2.weights)  # same initialization\n",
    "        data = random.rand(100,2)\n",
    "        som1 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        som1.train_random(data,10)\n",
    "        som2 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        som2.train_random(data,10)\n",
    "        assert_array_almost_equal(som1.weights,som2.weights)  # same state after training\n",
    "\n",
    "    def test_train_batch(self):\n",
    "        som = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        data = array([[4, 2], [3, 1]])\n",
    "        q1 = som.quantization_error(data)\n",
    "        som.train_batch(data, 10)\n",
    "        assert q1 > som.quantization_error(data)\n",
    "\n",
    "    def test_train_random(self):\n",
    "        som = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        data = array([[4, 2], [3, 1]])\n",
    "        q1 = som.quantization_error(data)\n",
    "        som.train_random(data, 10)\n",
    "        assert q1 > som.quantization_error(data)\n",
    "\n",
    "    def test_random_weights_init(self):\n",
    "        som = MiniSom(2, 2, 2, random_seed=1)\n",
    "        som.random_weights_init(array([[1.0, .0]]))\n",
    "        for w in som.weights:\n",
    "            assert_array_equal(w[0], array([1.0, .0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Som:\n",
    "    def __init__(self, x=20, y=None):\n",
    "        '''\n",
    "            parameter: x, y, size of the grid,\n",
    "                     default: 20x20, if y is not supplied, y is the same as x\n",
    "        '''\n",
    "        self.som = None\n",
    "        self.map_x = x\n",
    "        if not y:\n",
    "            self.map_y = x # square grid\n",
    "        else:\n",
    "            self.map_y = y\n",
    "\n",
    "    def fit(self, X):\n",
    "        '''\n",
    "            Fit SOM to the input data.\n",
    "            Parameters: X = a numpy array and it should contain\n",
    "                            all columns as features and any manually\n",
    "                            labeled columns should be removed before\n",
    "                            calling this function.\n",
    "        '''\n",
    "        nb_features = X.shape[1] # number of features\n",
    "        som = MiniSom(x = self.map_x, y = self.map_y, input_len = nb_features, sigma = 1.0, learning_rate = 0.5)\n",
    "        som.random_weights_init(X)\n",
    "        som.train_random(data = X, num_iteration = 2000)\n",
    "        dm = som.distance_map()\n",
    "        mid = []\n",
    "        for x in X:\n",
    "            w = som.winner(x)\n",
    "            (x,y) = w\n",
    "            mid.append(dm[x][y])\n",
    "\n",
    "        self.dm = dm\n",
    "        self.som = som\n",
    "        self.grid = (self.map_x, self.map_y)\n",
    "        self.mid = mid\n",
    "\n",
    "    def predict(self, threshold = 0.02, anomaly_label=0):\n",
    "        '''Predict data as normal or anomalous based upon mean inter-neuron distance.\n",
    "             Need to call fit() before calling this.\n",
    "             Parameters: threshold = the threshold (default = 0.02) that is used to\n",
    "                                     determine if normal = 1 (when mid <= threshold),\n",
    "                                     or anomalous = 0 otherwise.\n",
    "                         anomaly_label = the value to label anomalies, default = 0'''\n",
    "        if self.som is None:\n",
    "            raise Exception('Call fit() before calling this')\n",
    "\n",
    "        y_pred = []\n",
    "        for m in self.mid:\n",
    "            normal = (1 if m <= threshold else anomaly_label)\n",
    "            y_pred.append(normal)\n",
    "        return y_pred\n",
    "\n",
    "    def plot_marker(self, xy, m, c):\n",
    "        plot(xy[0] + 0.5,\n",
    "             xy[1] + 0.5,\n",
    "             m,\n",
    "             markeredgecolor = c,\n",
    "             markerfacecolor = 'None',\n",
    "             markersize = 10,\n",
    "             markeredgewidth = 2)\n",
    "\n",
    "    def plot_distance_map_labels(self, t, X, Y):\n",
    "        '''Plots distance map with labels. Need to call fit() before calling this.\n",
    "             Parameters: X = input features\n",
    "                         Y = labels, 1 = normal, 0 = anomalous.'''\n",
    "        if self.som is None:\n",
    "            raise Exception('Call fit() before calling this')\n",
    "\n",
    "        red_set = set() # normal instances\n",
    "        green_set = set() # anomalous instances\n",
    "        for i, x in enumerate(X):\n",
    "            w = self.som.winner(x)\n",
    "            if int(Y[i]) == 0:\n",
    "                red_set.add(w)\n",
    "            else:\n",
    "                green_set.add(w)\n",
    "        bone()\n",
    "        pcolor(self.dm.T)\n",
    "        colorbar()\n",
    "        (map_x, map_y) = self.grid\n",
    "        for x in range(map_x):\n",
    "            for y in range(map_y):\n",
    "                xy = (x,y)\n",
    "                if (xy in red_set) and (xy in green_set):\n",
    "                    self.plot_marker(xy, 'h', 'y')\n",
    "                elif xy in red_set:\n",
    "                    self.plot_marker(xy, 'o', 'r')\n",
    "                elif xy in green_set:\n",
    "                    self.plot_marker(xy, 's', 'g')\n",
    "                else:\n",
    "                    pass #plot_marker(xy, 'v', 'b')\n",
    "        title(t)\n",
    "        show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_teda_obj(observation, k=None, mean=None, var=None, ecc=None):\n",
    "    teda = {}\n",
    "    if not k:\n",
    "        k = 1\n",
    "        mean = observation\n",
    "        var = 0\n",
    "        ecc = 1\n",
    "    else:\n",
    "        if mean is None or var is None or ecc is None:\n",
    "            assert False, 'mean, variance and ecc values are required'\n",
    "\n",
    "    teda['k'] = k\n",
    "    teda['observation'] = observation\n",
    "    teda['mean'] = mean\n",
    "    teda['var'] = var\n",
    "    teda['eccentricity'] = ecc\n",
    "    teda['typicality'] = 1.0 - ecc\n",
    "    teda['norm_eccentricity'] = teda['eccentricity'] / 2.0\n",
    "    teda['norm_typicality'] = teda['typicality'] / (k - 2.0)\n",
    "    teda['outlier'] = 1.0 if teda['norm_eccentricity'] > (1.0 / k) else 0.0\n",
    "    teda['normal'] = 1.0 if teda['outlier'] < 0.5 else 0.0\n",
    "    teda['normal_bool'] = True if teda['normal'] > 0.5 else False\n",
    "    teda['ecc_threshold'] = 1.0 / k\n",
    "\n",
    "    return teda\n",
    "\n",
    "\n",
    "def calc_teda_single(observation, teda = None):\n",
    "    if not teda:\n",
    "        teda = gen_teda_obj(observation)\n",
    "    else:\n",
    "        k = teda['k'] + 1.0\n",
    "        mean = teda['mean']\n",
    "        var = teda['var']\n",
    "\n",
    "        # Calculate the running mean value\n",
    "        mean =  (((k - 1)  / k) * mean) + ((1 / k) * observation)\n",
    "\n",
    "        # Calculate the running mean value\n",
    "        var = (((k - 1) / k) * var) + (1 / (k - 1)) * np.linalg.norm(observation - mean)\n",
    "\n",
    "        # Calculate the running eccentricity value\n",
    "        ecc = (1 / k) +  (np.linalg.norm(mean - observation) / (k * var))\n",
    "\n",
    "        teda = gen_teda_obj(observation, k, mean, var, ecc)\n",
    "\n",
    "    return teda\n",
    "\n",
    "def calc_teda(X):\n",
    "    teda = None\n",
    "    teda_output = []\n",
    "    rows = X.shape[0]\n",
    "    for i in range(rows):\n",
    "        teda = calc_teda_single(X[i,:], teda)\n",
    "        teda_output.append(teda['normal'])\n",
    "\n",
    "    return teda_output\n",
    "\n",
    "def dytokinesis(X, X_norm, X_anom):\n",
    "    teda_norm = None\n",
    "    teda_anom = None\n",
    "    y_pred = []\n",
    "    rows = X.shape[0]\n",
    "    rows_norm = X_norm.shape[0]\n",
    "    rows_anom = X_anom.shape[0]\n",
    "    for i in range(rows_norm):\n",
    "        teda_norm = calc_teda_single(X_norm[i,:], teda_norm)\n",
    "\n",
    "    for i in range(rows_anom):\n",
    "        teda_anom = calc_teda_single(X_anom[i,:], teda_anom)\n",
    "\n",
    "    for i in range(rows):\n",
    "        teda_norm_tmp = calc_teda_single(X[i,:], teda_norm)\n",
    "        teda_anom_tmp = calc_teda_single(X[i,:], teda_anom)\n",
    "        \n",
    "        if (teda_norm_tmp['normal_bool'] == True):\n",
    "            y_pred.append(1.0)\n",
    "            teda_norm = teda_norm_tmp\n",
    "        else:\n",
    "            y_pred.append(0.0)\n",
    "            teda_anom = teda_anom_tmp\n",
    "\n",
    "    return(np.array(y_pred))\n",
    "\n",
    "class TEDA:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        return np.array(calc_teda(X))\n",
    "\n",
    "class Dytokinesis:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def predict(self, X, X_norm, X_anom):\n",
    "        return dytokinesis(X, X_norm, X_anom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_execution_time(X, X_norm, X_anom):\n",
    "    state = np.random.RandomState(42)\n",
    "    # Three different types of detectors\n",
    "    detectors = {\n",
    "        \"TEDA\":TEDA(),\n",
    "        \"Dytokinesis\": Dytokinesis(),\n",
    "        \"SOM\": Som(),\n",
    "        \"iForest\":IsolationForest(n_estimators=100, max_samples=len(X), \n",
    "                                           contamination=0.5,random_state=state, verbose=0),\n",
    "        \"LOF\":LocalOutlierFactor(n_neighbors=20, algorithm='auto', \n",
    "                                                  leaf_size=30, metric='minkowski',\n",
    "                                                  p=2, metric_params=None, contamination=0.5),\n",
    "        \"OcSVM\":OneClassSVM(kernel='rbf', degree=3, gamma=0.1,nu=0.05, \n",
    "                                             max_iter=-1)\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i, (name,detector) in enumerate(detectors.items()):\n",
    "        start = timer()\n",
    "        if name == \"TEDA\":\n",
    "            y_pred = detector.predict(X)\n",
    "        elif name == \"Dytokinesis\":\n",
    "            y_pred = detector.predict(X, X_norm, X_anom)\n",
    "        elif name == \"SOM\":\n",
    "            detector.fit(X)\n",
    "            y_pred = detector.predict(0.5)\n",
    "        elif name == \"LOF\":\n",
    "            y_pred = detector.fit_predict(X)\n",
    "            scores_prediction = detector.negative_outlier_factor_\n",
    "        elif name == \"OcSVM\":\n",
    "            detector.fit(X_norm)\n",
    "            y_pred = detector.predict(X)\n",
    "        else: # iForest\n",
    "            detector.fit(X)\n",
    "            scores_prediction = detector.decision_function(X)\n",
    "            y_pred = detector.predict(X)\n",
    "        end = timer()\n",
    "        execution_time = end - start\n",
    "        results.append((name, execution_time))\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 1.benign.csv rows = 49548\n",
      "processing 1.gafgyt.combo.csv rows = 59718\n",
      "processing 1.gafgyt.junk.csv rows = 29068\n",
      "processing 1.gafgyt.scan.csv rows = 29849\n",
      "processing 1.gafgyt.tcp.csv rows = 92141\n",
      "processing 1.gafgyt.udp.csv rows = 105874\n",
      "processing 1.mirai.ack.csv rows = 102195\n",
      "processing 1.mirai.scan.csv rows = 107685\n",
      "processing 1.mirai.syn.csv rows = 122573\n",
      "processing 1.mirai.udp.csv rows = 237665\n",
      "processing 1.mirai.udpplain.csv rows = 81982\n",
      "Danmini_Doorbell\n",
      "count_norm: 4954, count_anom: 96875, count_total: 101829\n",
      "method,execution_time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEDA,4.188347\n",
      "Dytokinesis,12.166115\n",
      "SOM,221.490781\n",
      "iForest,41.183936\n",
      "LOF,218.019847\n",
      "OcSVM,6.724887\n",
      "processing 2.benign.csv rows = 13113\n",
      "processing 2.gafgyt.combo.csv rows = 53012\n",
      "processing 2.gafgyt.junk.csv rows = 30312\n",
      "processing 2.gafgyt.scan.csv rows = 27494\n",
      "processing 2.gafgyt.tcp.csv rows = 95021\n",
      "processing 2.gafgyt.udp.csv rows = 104791\n",
      "processing 2.mirai.ack.csv rows = 113285\n",
      "processing 2.mirai.scan.csv rows = 43192\n",
      "processing 2.mirai.syn.csv rows = 116807\n",
      "processing 2.mirai.udp.csv rows = 151481\n",
      "processing 2.mirai.udpplain.csv rows = 87368\n",
      "Ecobee_Thermostat\n",
      "count_norm: 1311, count_anom: 82276, count_total: 83587\n",
      "method,execution_time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEDA,3.467553\n",
      "Dytokinesis,9.979282\n",
      "SOM,186.051602\n",
      "iForest,29.463849\n",
      "LOF,65.687631\n",
      "OcSVM,12.053846\n",
      "processing 3.benign.csv rows = 39100\n",
      "processing 3.gafgyt.combo.csv rows = 53014\n",
      "processing 3.gafgyt.junk.csv rows = 29797\n",
      "processing 3.gafgyt.scan.csv rows = 28120\n",
      "processing 3.gafgyt.tcp.csv rows = 101536\n",
      "processing 3.gafgyt.udp.csv rows = 103933\n",
      "Ennio_Doorbell\n",
      "count_norm: 3910, count_anom: 31640, count_total: 35550\n",
      "method,execution_time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEDA,1.493648\n",
      "Dytokinesis,4.211493\n",
      "SOM,93.659173\n",
      "iForest,12.305847\n",
      "LOF,67.613441\n",
      "OcSVM,8.578058\n",
      "processing 4.benign.csv rows = 175240\n",
      "processing 4.gafgyt.combo.csv rows = 58152\n",
      "processing 4.gafgyt.junk.csv rows = 28349\n",
      "processing 4.gafgyt.scan.csv rows = 27859\n",
      "processing 4.gafgyt.tcp.csv rows = 92581\n",
      "processing 4.gafgyt.udp.csv rows = 105782\n",
      "processing 4.mirai.ack.csv rows = 91123\n",
      "processing 4.mirai.scan.csv rows = 103621\n",
      "processing 4.mirai.syn.csv rows = 118128\n",
      "processing 4.mirai.udp.csv rows = 217034\n",
      "processing 4.mirai.udpplain.csv rows = 80808\n",
      "Philips_B120N10_Baby_Monitor\n",
      "count_norm: 17524, count_anom: 92343, count_total: 109867\n",
      "method,execution_time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEDA,4.588066\n",
      "Dytokinesis,13.134701\n",
      "SOM,237.077884\n",
      "iForest,37.396427\n",
      "LOF,447.609640\n",
      "OcSVM,89.908316\n",
      "processing 5.benign.csv rows = 62154\n",
      "processing 5.gafgyt.combo.csv rows = 61380\n",
      "processing 5.gafgyt.junk.csv rows = 30898\n",
      "processing 5.gafgyt.scan.csv rows = 29297\n",
      "processing 5.gafgyt.tcp.csv rows = 104510\n",
      "processing 5.gafgyt.udp.csv rows = 104011\n",
      "processing 5.mirai.ack.csv rows = 60554\n",
      "processing 5.mirai.scan.csv rows = 96781\n",
      "processing 5.mirai.syn.csv rows = 65746\n",
      "processing 5.mirai.udp.csv rows = 156248\n",
      "processing 5.mirai.udpplain.csv rows = 56681\n",
      "Provision_PT_737E_Security_Camera\n",
      "count_norm: 6215, count_anom: 76610, count_total: 82826\n",
      "method,execution_time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEDA,3.405566\n",
      "Dytokinesis,9.779587\n",
      "SOM,187.088650\n",
      "iForest,32.722520\n",
      "LOF,90.057478\n",
      "OcSVM,14.957004\n",
      "processing 6.benign.csv rows = 98514\n",
      "processing 6.gafgyt.combo.csv rows = 57530\n",
      "processing 6.gafgyt.junk.csv rows = 29068\n",
      "processing 6.gafgyt.scan.csv rows = 28397\n",
      "processing 6.gafgyt.tcp.csv rows = 89387\n",
      "processing 6.gafgyt.udp.csv rows = 104658\n",
      "processing 6.mirai.ack.csv rows = 57997\n",
      "processing 6.mirai.scan.csv rows = 97096\n",
      "processing 6.mirai.syn.csv rows = 61851\n",
      "processing 6.mirai.udp.csv rows = 158608\n",
      "processing 6.mirai.udpplain.csv rows = 53785\n",
      "Provision_PT_838_Security_Camera\n",
      "count_norm: 9851, count_anom: 73837, count_total: 83689\n",
      "method,execution_time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEDA,3.552170\n",
      "Dytokinesis,9.918269\n",
      "SOM,187.104829\n",
      "iForest,32.061836\n",
      "LOF,160.823234\n",
      "OcSVM,23.945682\n",
      "processing 7.benign.csv rows = 52150\n",
      "processing 7.gafgyt.combo.csv rows = 58669\n",
      "processing 7.gafgyt.junk.csv rows = 28305\n",
      "processing 7.gafgyt.scan.csv rows = 27698\n",
      "processing 7.gafgyt.tcp.csv rows = 97783\n",
      "processing 7.gafgyt.udp.csv rows = 110617\n",
      "Samsung_SNH_1011_N_Webcam\n",
      "count_norm: 5215, count_anom: 32307, count_total: 37522\n",
      "method,execution_time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEDA,1.560303\n",
      "Dytokinesis,4.477776\n",
      "SOM,94.505453\n",
      "iForest,14.071674\n",
      "LOF,93.392772\n",
      "OcSVM,2.692025\n",
      "processing 8.benign.csv rows = 46585\n",
      "processing 8.gafgyt.combo.csv rows = 54283\n",
      "processing 8.gafgyt.junk.csv rows = 28579\n",
      "processing 8.gafgyt.scan.csv rows = 27825\n",
      "processing 8.gafgyt.tcp.csv rows = 88816\n",
      "processing 8.gafgyt.udp.csv rows = 103720\n",
      "processing 8.mirai.ack.csv rows = 111480\n",
      "processing 8.mirai.scan.csv rows = 45930\n",
      "processing 8.mirai.syn.csv rows = 125715\n",
      "processing 8.mirai.udp.csv rows = 151879\n",
      "processing 8.mirai.udpplain.csv rows = 78244\n",
      "SimpleHome_XCS7_1002_WHT_Security_Camera\n",
      "count_norm: 4658, count_anom: 81647, count_total: 86305\n",
      "method,execution_time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEDA,3.523293\n",
      "Dytokinesis,10.178948\n",
      "SOM,190.803531\n",
      "iForest,30.806797\n",
      "LOF,214.044284\n",
      "OcSVM,15.833569\n",
      "processing 9.benign.csv rows = 19528\n",
      "processing 9.gafgyt.combo.csv rows = 59398\n",
      "processing 9.gafgyt.junk.csv rows = 27413\n",
      "processing 9.gafgyt.scan.csv rows = 28572\n",
      "processing 9.gafgyt.tcp.csv rows = 98075\n",
      "processing 9.gafgyt.udp.csv rows = 102980\n",
      "processing 9.mirai.ack.csv rows = 107187\n",
      "processing 9.mirai.scan.csv rows = 43674\n",
      "processing 9.mirai.syn.csv rows = 122479\n",
      "processing 9.mirai.udp.csv rows = 157084\n",
      "processing 9.mirai.udpplain.csv rows = 84436\n",
      "SimpleHome_XCS7_1003_WHT_Security_Camera\n",
      "count_norm: 1952, count_anom: 83129, count_total: 85082\n",
      "method,execution_time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEDA,3.482007\n",
      "Dytokinesis,10.120048\n",
      "SOM,189.226799\n",
      "iForest,31.025773\n",
      "LOF,135.341196\n",
      "OcSVM,7.063111\n"
     ]
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    device_index = i\n",
    "    device_id = device_index + 1\n",
    "    device_name = dn_nbaiot[device_index]\n",
    "    (X, y, Xdf) = get_nbaiot_device_data(device_id)\n",
    "    X_std = StandardScaler().fit_transform(X)\n",
    "    indices_norm = np.where(y >= 0.5)\n",
    "    indices_anom = np.where(y <= 0.5)\n",
    "    X_norm_all = X_std[indices_norm]\n",
    "    X_anom_all = X_std[indices_anom]\n",
    "    cout_frac = 0.10\n",
    "    count_all = int(cout_frac * float(X_std.shape[0]))\n",
    "    count_norm = int(cout_frac * float(X_norm_all.shape[0]))\n",
    "    count_anom = int(cout_frac * float(X_anom_all.shape[0]))\n",
    "    X_norm = X_norm_all[0:count_norm,:]\n",
    "    X_anom = X_anom_all[0:count_anom,:]\n",
    "    X_std = X_std[0:count_all,:]\n",
    "    print(device_name)\n",
    "    print(\"count_norm: {}, count_anom: {}, count_total: {}\".format(X_norm.shape[0], X_anom.shape[0], X_std.shape[0]))\n",
    "    print(\"method,execution_time\")\n",
    "    results = measure_execution_time(X_std, X_norm, X_anom)\n",
    "    for result in results:\n",
    "        (name, execution_time) = result\n",
    "        print(f'{name},{execution_time:.6f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
